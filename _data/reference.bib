@ARTICLE{2022TIP_An_selfsupervise,
  author={An, Cheolhong and Wang, Yiqian and Zhang, Junkang and Nguyen, Truong Q.},
  journal={IEEE Transactions on Image Processing}, 
  title={Self-Supervised Rigid Registration for Multimodal Retinal Images}, 
  year={2022},
  volume={31},
  number={},
  pages={5733-5747},
  abstract={The ability to accurately overlay one modality retinal image to another is critical in ophthalmology. Our previous framework achieved the state-of-the-art results for multimodal retinal image registration. However, it requires human-annotated labels due to the supervised approach of the previous work. In this paper, we propose a self-supervised multimodal retina registration method to alleviate the burdens of time and expense to prepare for training data, that is, aiming to automatically register multimodal retinal images without any human annotations. Specially, we focus on registering color fundus images with infrared reflectance and fluorescein angiography images, and compare registration results with several conventional and supervised and unsupervised deep learning methods. From the experimental results, the proposed self-supervised framework achieves a comparable accuracy comparing to the state-of-the-art supervised learning method in terms of registration accuracy and Dice coefficient.},
  keywords={},
  doi={10.1109/TIP.2022.3201476},
  ISSN={1941-0042},
  month={},}

@ARTICLE{2022TIP_Zhang_twostep,
  author={Zhang, Junkang and Wang, Yiqian and Dai, Ji and Cavichini, Melina and Bartsch, Dirk-Uwe G. and Freeman, William R. and Nguyen, Truong Q. and An, Cheolhong},
  journal={IEEE Transactions on Image Processing}, 
  title={Two-Step Registration on Multi-Modal Retinal Images via Deep Neural Networks}, 
  year={2022},
  volume={31},
  number={},
  pages={823-838},
  abstract={Multi-modal retinal image registration plays an important role in the ophthalmological diagnosis process. The conventional methods lack robustness in aligning multi-modal images of various imaging qualities. Deep-learning methods have not been widely developed for this task, especially for the coarse-to-fine registration pipeline. To handle this task, we propose a two-step method based on deep convolutional networks, including a coarse alignment step and a fine alignment step. In the coarse alignment step, a global registration matrix is estimated by three sequentially connected networks for vessel segmentation, feature detection and description, and outlier rejection, respectively. In the fine alignment step, a deformable registration network is set up to find pixel-wise correspondence between a target image and a coarsely aligned image from the previous step to further improve the alignment accuracy. Particularly, an unsupervised learning framework is proposed to handle the difficulties of inconsistent modalities and lack of labeled training data for the fine alignment step. The proposed framework first changes multi-modal images into a same modality through modality transformers, and then adopts photometric consistency loss and smoothness loss to train the deformable registration network. The experimental results show that the proposed method achieves state-of-the-art results in Dice metrics and is more robust in challenging cases.},
  keywords={},
  doi={10.1109/TIP.2021.3135708},
  ISSN={1941-0042},
  month={},}

@ARTICLE{2021TIP_Wang_robust,
  author={Wang, Yiqian and Zhang, Junkang and Cavichini, Melina and Bartsch, Dirk-Uwe G. and Freeman, William R. and Nguyen, Truong Q. and An, Cheolhong},
  journal={IEEE Transactions on Image Processing}, 
  title={Robust Content-Adaptive Global Registration for Multimodal Retinal Images Using Weakly Supervised Deep-Learning Framework}, 
  year={2021},
  volume={30},
  number={},
  pages={3167-3178},
  abstract={Multimodal retinal imaging plays an important role in ophthalmology. We propose a content-adaptive multimodal retinal image registration method in this paper that focuses on the globally coarse alignment and includes three weakly supervised neural networks for vessel segmentation, feature detection and description, and outlier rejection. We apply the proposed framework to register color fundus images with infrared reflectance and fluorescein angiography images, and compare it with several conventional and deep learning methods. Our proposed framework demonstrates a significant improvement in robustness and accuracy reflected by a higher success rate and Dice coefficient compared with other methods.},
  keywords={},
  doi={10.1109/TIP.2021.3058570},
  ISSN={1941-0042},
  month={},}

@INPROCEEDINGS{2021EMBC_Zhang_distortion,
  author={Zhang, Junkang and Wang, Yiqian and Bartsch, Dirk-Uwe G. and Freeman, William R. and Nguyen, Truong Q. and An, Cheolhong},
  booktitle={2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)}, 
  title={Perspective Distortion Correction for Multi-Modal Registration between Ultra-Widefield and Narrow-Angle Retinal Images}, 
  year={2021},
  volume={},
  number={},
  pages={4086-4091},
  abstract={Multi-modal retinal image registration between 2D Ultra-Widefield (UWF) and narrow-angle (NA) images has not been well-studied, since most existing methods mainly focus on NA image alignment. The stereographic projection model used in UWF imaging causes strong distortions in peripheral areas, which leads to inferior alignment quality. We propose a distortion correction method that remaps the UWF images based on estimated camera view points of NA images. In addition, we set up a CNN-based registration pipeline for UWF and NA images, which consists of the distortion correction method and three networks for vessel segmentation, feature detection and matching, and outlier rejection. Experimental results on our collected dataset shows the effectiveness of the proposed pipeline and the distortion correction method.},
  keywords={},
  doi={10.1109/EMBC46164.2021.9631084},
  ISSN={2694-0604},
  month={Nov},}

@ARTICLE{2020Access_Wang_correlation,
  author={Wang, Yiqian and Zhang, Junkang and Cavichini, Melina and Bartsch, Dirk-Uwe G. and Freeman, William R. and Nguyen, Truong Q. and An, Cheolhong},
  journal={IEEE Access}, 
  title={Study on Correlation Between Subjective and Objective Metrics for Multimodal Retinal Image Registration}, 
  year={2020},
  volume={8},
  number={},
  pages={190897-190905},
  abstract={Retinal imaging is crucial in diagnosing and treating retinal diseases, and multimodal retinal image registration constitutes a major advance in understanding retinal diseases. Despite the fact that many methods have been proposed for the registration task, the evaluation metrics for successful registration have not been thoroughly studied. In this article, we present a comprehensive overview of the existing evaluation metrics for multimodal retinal image registration, and compare the similarity between the subjective grade of ophthalmologists and various objective metrics. The Pearson's correlation coefficient and the corresponding confidence interval are used to evaluate metrics similarity. It is found that the binary and soft Dice coefficient on the segmented vessel can achieve the highest correlation with the subjective grades compared to other keypoint-supervised or unsupervised metrics. The paper established an objective metric that is highly correlated with the subjective evaluation of the ophthalmologists, which has never been studied before. The experimental results would build a connection between ophthalmology and image processing literature, and the findings may provide a good insight for researchers who investigate retinal image registration, retinal image segmentation and image domain transformation.},
  keywords={},
  doi={10.1109/ACCESS.2020.3032348},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{2020ICASSP_Wang_robust,
  author={Wang, Yiqian and Zhang, Junkang and An, Cheolhong and Cavichini, Melina and Jhingan, Mahima and Amador-Patarroyo, Manuel J. and Long, Christopher P. and Bartsch, Dirk-Uwe G. and Freeman, William R. and Nguyen, Truong Q.},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A Segmentation Based Robust Deep Learning Framework for Multimodal Retinal Image Registration}, 
  year={2020},
  volume={},
  number={},
  pages={1369-1373},
  abstract={Multimodal image registration plays an important role in diagnosing and treating ophthalmologic diseases. In this paper, a deep learning framework for multimodal retinal image registration is proposed. The framework consists of a segmentation network, feature detection and description network, and an outlier rejection network, which focuses only on the globally coarse alignment step using the perspective transformation. We apply the proposed framework to register color fundus images with infrared reflectance images and compare it with the state-of-the-art conventional and learning-based approaches. The proposed framework demonstrates a significant improvement in robustness and accuracy reflected by a higher success rate and Dice coefficient compared to other coarse alignment methods.},
  keywords={},
  doi={10.1109/ICASSP40776.2020.9054077},
  ISSN={2379-190X},
  month={May},}

@INPROCEEDINGS{2019ICIP_Zhang_style,
  author={Zhang, Junkang and An, Cheolhong and Dai, Ji and Amador, Manuel and Bartsch, Dirk-Uwe and Borooah, Shyamanga and Freeman, William R. and Nguyen, Truong Q.},
  booktitle={2019 IEEE International Conference on Image Processing (ICIP)}, 
  title={Joint Vessel Segmentation and Deformable Registration on Multi-Modal Retinal Images Based on Style Transfer}, 
  year={2019},
  volume={},
  number={},
  pages={839-843},
  abstract={In multi-modal retinal image registration task, there are two major challenges, i.e., poor performance in finding correspondence due to inconsistent features, and lack of labeled data for training learning-based models. In this paper, we propose a joint vessel segmentation and deformable registration model based on CNN for this task, built under the framework of weakly supervised style transfer learning and perceptual loss. In vessel segmentation, a style loss guides the model to generate segmentation maps that look authentic, and helps transform images of different modalities into consistent representations. In deformable registration, a content loss helps find dense correspondence for multi-modal images based on their consistent representations, and improves the segmentation results simultaneously. Experiment results show that our model has better performance than other deformable registration methods in both quantitative and visual evaluations, and the segmentation results also help the rigid transformation1.},
  keywords={},
  doi={10.1109/ICIP.2019.8802932},
  ISSN={2381-8549},
  month={Sep.},}

@article{2020TVST_Cavichini_overlay,
    author = {Cavichini, Melina and An, Cheolhong and Bartsch, Dirk-Uwe G. and Jhingan, Mahima and Amador-Patarroyo, Manuel J. and Long, Christopher P. and Zhang, Junkang and Wang, Yiqian and Chan, Alison X. and Madala, Samantha and Nguyen, Truong and Freeman, William R.},
    title = "{Artificial Intelligence for Automated Overlay of Fundus Camera and Scanning Laser Ophthalmoscope Images}",
    journal = {Translational Vision Science & Technology},
    volume = {9},
    number = {2},
    pages = {56-56},
    year = {2020},
    month = {10},
    abstract = "{   The purpose of this study was to evaluate the ability to align two types of retinal images taken on different platforms; color fundus (CF) photographs and infrared scanning laser ophthalmoscope (IR SLO) images using mathematical warping and artificial intelligence (AI).    We collected 109 matched pairs of CF and IR SLO images. An AI algorithm utilizing two separate networks was developed. A style transfer network (STN) was used to segment vessel structures. A registration network was used to align the segmented images to each. Neither network used a ground truth dataset. A conventional image warping algorithm was used as a control. Software displayed image pairs as a 5 × 5 checkerboard grid composed of alternating subimages. This technique permitted vessel alignment determination by human observers and 5 masked graders evaluated alignment by the AI and conventional warping in 25 fields for each image.    Our new AI method was superior to conventional warping at generating vessel alignment as judged by masked human graders (P \\&lt; 0.0001). The average number of good/excellent matches increased from 90.5\\% to 94.4\\% with AI method.    AI permitted a more accurate overlay of CF and IR SLO images than conventional mathematical warping. This is a first step toward developing an AI that could allow overlay of all types of fundus images by utilizing vascular landmarks.    The ability to align and overlay imaging data from multiple instruments and manufacturers will permit better analysis of this complex data helping understand disease and predict treatment.  }",
    issn = {2164-2591},
    doi = {10.1167/tvst.9.2.56},
    url = {https://doi.org/10.1167/tvst.9.2.56},
    eprint = {https://arvojournals.org/arvo/content\_public/journal/tvst/938366/i2164-2591-9-2-56\_1603109585.80174.pdf},
}



