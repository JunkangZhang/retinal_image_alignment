@INPROCEEDINGS{2023ICIP_Zhang_uwfeyeball,
  author={Zhang, Junkang and Wen, Bo and Kalaw, Fritz Gerald P. and Cavichini, Melina and Bartsch, Dirk-Uwe G. and Freeman, William R. and Nguyen, Truong Q. and An, Cheolhong},
  booktitle={2023 IEEE International Conference on Image Processing (ICIP)},
  title={Accurate Registration between Ultra-Wide-Field and Narrow Angle Retina Images with 3D Eyeball Shape Optimization},
  year={2023},
  volume={},
  number={},
  pages={2750-2754},
  doi={10.1109/ICIP49359.2023.10223163},
  abstract={The Ultra-Wide-Field (UWF) retina images have attracted wide attentions in recent years in the study of retina. However, accurate registration between the UWF images and the other types of retina images could be challenging due to the distortion in the peripheral areas of an UWF image, which a 2D warping procedure can not handle. In this paper, we propose a novel 3D distortion correction method which sets up a 3D projection model and optimizes a dense 3D retina mesh to correct the distortion in the UWF image. The corrected UWF image can then be accurately aligned to the target image using 2D alignment methods. The experimental results show that our proposed method outperforms the state-of-the-art method by 30\\%.}}

@article{2023Eye_Kalaw_composite,
  title={Ultra-wide field and new wide field composite retinal image registration with AI-enabled pipeline and 3D distortion correction algorithm},
  author={Kalaw, Fritz Gerald P. and Cavichini, Melina and Zhang, Junkang and Wen, Bo and Lin, Andrew C. and Heinke, Anna and Nguyen, Truong and An, Cheolhong and Bartsch, Dirk-Uwe G. and Cheng, Lingyun and Freeman, William R.},
  journal={Eye},
  pages={1--7},
  year={2023},
  publisher={Nature Publishing Group UK London},
  doi={10.1038/s41433-023-02868-3},
  abstract={This study aimed to compare a new Artificial Intelligence (AI) method to conventional mathematical warping in accurately overlaying peripheral retinal vessels from two different imaging devices: confocal scanning laser ophthalmoscope (cSLO) wide-field images and SLO ultra-wide field images. Images were captured using the Heidelberg Spectralis 55-degree field-of-view and Optos ultra-wide field. The conventional mathematical warping was performed using Random Sample Consensusâ€”Sample and Consensus sets (RANSAC-SC). This was compared to an AI alignment algorithm based on a one-way forward registration procedure consisting of full Convolutional Neural Networks (CNNs) with Outlier Rejection (OR CNN), as well as an iterative 3D camera pose optimization process (OR CNN + Distortion Correction [DC]). Images were provided in a checkerboard pattern, and peripheral vessels were graded in four quadrants based on alignment to the adjacent box. A total of 660 boxes were analysed from 55 eyes. Dice scores were compared between the three methods (RANSAC-SC/OR CNN/OR CNN + DC): 0.3341/0.4665/4784 for fold 1-2 and 0.3315/0.4494/4596 for fold 2-1 in composite images. The images composed using the OR CNN + DC have a median rating of 4 (out of 5) versus 2 using RANSAC-SC. The odds of getting a higher grading level are 4.8 times higher using our OR CNN + DC than RANSAC-SC (p \\&lt; 0.0001). Peripheral retinal vessel alignment performed better using our AI algorithm than RANSAC-SC. This may help improve co-localizing retinal anatomy and pathology with our algorithm.}}

@article{2023OSLIR_Cavichini_sidebyside,
  title={Accuracy and Time Comparison Between Side-by-Side and Artificial Intelligence Overlayed Images},
  author={Cavichini, Melina and Bartsch, Dirk-Uwe G and Warter, Alexandra and Singh, Sumit and An, Cheolhong and Wang, Yiqian and Zhang, Junkang and Nguyen, Truong and Freeman, William R},
  journal={Ophthalmic Surgery, Lasers and Imaging Retina},
  volume={54},
  number={2},
  pages={108--113},
  year={2023},
  publisher={SLACK Incorporated Thorofare, NJ},
  doi={10.3928/23258160-20230130-03},
  abstract={The purpose of this study was to evaluate the accuracy and the time to find a lesion, taken in different platforms, color fundus photographs and infrared scanning laser ophthalmoscope images, using the traditional side-by-side (SBS) colocalization technique to an artificial intelligence (AI)-assisted technique. Fifty-three pathological lesions were studied in 11 eyes. Images were aligned using SBS and AI overlaid methods. The location of each color fundus lesion on the corresponding infrared scanning laser ophthalmoscope image was analyzed twice, one time for each method, on different days, for two specialists, in random order. The outcomes for each method were measured and recorded by an independent observer. The colocalization AI method was superior to the conventional in accuracy and time (P \\&lt; .001), with a mean time to colocalize 37\\% faster. The error rate using AI was 0\\% compared with 18\\% in SBS measurements. AI permitted a more accurate and faster colocalization of pathologic lesions than the conventional method.}}


@ARTICLE{2022TIP_An_selfsupervise,
  author={An, Cheolhong and Wang, Yiqian and Zhang, Junkang and Nguyen, Truong Q.},
  journal={IEEE Transactions on Image Processing}, 
  title={Self-Supervised Rigid Registration for Multimodal Retinal Images}, 
  year={2022},
  volume={31},
  number={},
  pages={5733-5747},
  abstract={The ability to accurately overlay one modality retinal image to another is critical in ophthalmology. Our previous framework achieved the state-of-the-art results for multimodal retinal image registration. However, it requires human-annotated labels due to the supervised approach of the previous work. In this paper, we propose a self-supervised multimodal retina registration method to alleviate the burdens of time and expense to prepare for training data, that is, aiming to automatically register multimodal retinal images without any human annotations. Specially, we focus on registering color fundus images with infrared reflectance and fluorescein angiography images, and compare registration results with several conventional and supervised and unsupervised deep learning methods. From the experimental results, the proposed self-supervised framework achieves a comparable accuracy comparing to the state-of-the-art supervised learning method in terms of registration accuracy and Dice coefficient.},
  keywords={},
  doi={10.1109/TIP.2022.3201476},
  ISSN={1941-0042},
  month={},}

@ARTICLE{2022TIP_Zhang_twostep,
  author={Zhang, Junkang and Wang, Yiqian and Dai, Ji and Cavichini, Melina and Bartsch, Dirk-Uwe G. and Freeman, William R. and Nguyen, Truong Q. and An, Cheolhong},
  journal={IEEE Transactions on Image Processing}, 
  title={Two-Step Registration on Multi-Modal Retinal Images via Deep Neural Networks}, 
  year={2022},
  volume={31},
  number={},
  pages={823-838},
  abstract={Multi-modal retinal image registration plays an important role in the ophthalmological diagnosis process. The conventional methods lack robustness in aligning multi-modal images of various imaging qualities. Deep-learning methods have not been widely developed for this task, especially for the coarse-to-fine registration pipeline. To handle this task, we propose a two-step method based on deep convolutional networks, including a coarse alignment step and a fine alignment step. In the coarse alignment step, a global registration matrix is estimated by three sequentially connected networks for vessel segmentation, feature detection and description, and outlier rejection, respectively. In the fine alignment step, a deformable registration network is set up to find pixel-wise correspondence between a target image and a coarsely aligned image from the previous step to further improve the alignment accuracy. Particularly, an unsupervised learning framework is proposed to handle the difficulties of inconsistent modalities and lack of labeled training data for the fine alignment step. The proposed framework first changes multi-modal images into a same modality through modality transformers, and then adopts photometric consistency loss and smoothness loss to train the deformable registration network. The experimental results show that the proposed method achieves state-of-the-art results in Dice metrics and is more robust in challenging cases.},
  keywords={},
  doi={10.1109/TIP.2021.3135708},
  ISSN={1941-0042},
  month={},}


@ARTICLE{2021TIP_Wang_robust,
  author={Wang, Yiqian and Zhang, Junkang and Cavichini, Melina and Bartsch, Dirk-Uwe G. and Freeman, William R. and Nguyen, Truong Q. and An, Cheolhong},
  journal={IEEE Transactions on Image Processing}, 
  title={Robust Content-Adaptive Global Registration for Multimodal Retinal Images Using Weakly Supervised Deep-Learning Framework}, 
  year={2021},
  volume={30},
  number={},
  pages={3167-3178},
  abstract={Multimodal retinal imaging plays an important role in ophthalmology. We propose a content-adaptive multimodal retinal image registration method in this paper that focuses on the globally coarse alignment and includes three weakly supervised neural networks for vessel segmentation, feature detection and description, and outlier rejection. We apply the proposed framework to register color fundus images with infrared reflectance and fluorescein angiography images, and compare it with several conventional and deep learning methods. Our proposed framework demonstrates a significant improvement in robustness and accuracy reflected by a higher success rate and Dice coefficient compared with other methods.},
  keywords={},
  doi={10.1109/TIP.2021.3058570},
  ISSN={1941-0042},
  month={},}

@INPROCEEDINGS{2021EMBC_Zhang_distortion,
  author={Zhang, Junkang and Wang, Yiqian and Bartsch, Dirk-Uwe G. and Freeman, William R. and Nguyen, Truong Q. and An, Cheolhong},
  booktitle={2021 43rd Annual International Conference of the IEEE Engineering in Medicine & Biology Society (EMBC)}, 
  title={Perspective Distortion Correction for Multi-Modal Registration between Ultra-Widefield and Narrow-Angle Retinal Images}, 
  year={2021},
  volume={},
  number={},
  pages={4086-4091},
  abstract={Multi-modal retinal image registration between 2D Ultra-Widefield (UWF) and narrow-angle (NA) images has not been well-studied, since most existing methods mainly focus on NA image alignment. The stereographic projection model used in UWF imaging causes strong distortions in peripheral areas, which leads to inferior alignment quality. We propose a distortion correction method that remaps the UWF images based on estimated camera view points of NA images. In addition, we set up a CNN-based registration pipeline for UWF and NA images, which consists of the distortion correction method and three networks for vessel segmentation, feature detection and matching, and outlier rejection. Experimental results on our collected dataset shows the effectiveness of the proposed pipeline and the distortion correction method.},
  keywords={},
  doi={10.1109/EMBC46164.2021.9631084},
  ISSN={2694-0604},
  month={Nov},}


@ARTICLE{2020Access_Wang_correlation,
  author={Wang, Yiqian and Zhang, Junkang and Cavichini, Melina and Bartsch, Dirk-Uwe G. and Freeman, William R. and Nguyen, Truong Q. and An, Cheolhong},
  journal={IEEE Access}, 
  title={Study on Correlation Between Subjective and Objective Metrics for Multimodal Retinal Image Registration}, 
  year={2020},
  volume={8},
  number={},
  pages={190897-190905},
  abstract={Retinal imaging is crucial in diagnosing and treating retinal diseases, and multimodal retinal image registration constitutes a major advance in understanding retinal diseases. Despite the fact that many methods have been proposed for the registration task, the evaluation metrics for successful registration have not been thoroughly studied. In this article, we present a comprehensive overview of the existing evaluation metrics for multimodal retinal image registration, and compare the similarity between the subjective grade of ophthalmologists and various objective metrics. The Pearson's correlation coefficient and the corresponding confidence interval are used to evaluate metrics similarity. It is found that the binary and soft Dice coefficient on the segmented vessel can achieve the highest correlation with the subjective grades compared to other keypoint-supervised or unsupervised metrics. The paper established an objective metric that is highly correlated with the subjective evaluation of the ophthalmologists, which has never been studied before. The experimental results would build a connection between ophthalmology and image processing literature, and the findings may provide a good insight for researchers who investigate retinal image registration, retinal image segmentation and image domain transformation.},
  keywords={},
  doi={10.1109/ACCESS.2020.3032348},
  ISSN={2169-3536},
  month={},}

@INPROCEEDINGS{2020ICASSP_Wang_robust,
  author={Wang, Yiqian and Zhang, Junkang and An, Cheolhong and Cavichini, Melina and Jhingan, Mahima and Amador-Patarroyo, Manuel J. and Long, Christopher P. and Bartsch, Dirk-Uwe G. and Freeman, William R. and Nguyen, Truong Q.},
  booktitle={ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)}, 
  title={A Segmentation Based Robust Deep Learning Framework for Multimodal Retinal Image Registration}, 
  year={2020},
  volume={},
  number={},
  pages={1369-1373},
  abstract={Multimodal image registration plays an important role in diagnosing and treating ophthalmologic diseases. In this paper, a deep learning framework for multimodal retinal image registration is proposed. The framework consists of a segmentation network, feature detection and description network, and an outlier rejection network, which focuses only on the globally coarse alignment step using the perspective transformation. We apply the proposed framework to register color fundus images with infrared reflectance images and compare it with the state-of-the-art conventional and learning-based approaches. The proposed framework demonstrates a significant improvement in robustness and accuracy reflected by a higher success rate and Dice coefficient compared to other coarse alignment methods.},
  keywords={},
  doi={10.1109/ICASSP40776.2020.9054077},
  ISSN={2379-190X},
  month={May},}


@INPROCEEDINGS{2019ICIP_Zhang_style,
  author={Zhang, Junkang and An, Cheolhong and Dai, Ji and Amador, Manuel and Bartsch, Dirk-Uwe and Borooah, Shyamanga and Freeman, William R. and Nguyen, Truong Q.},
  booktitle={2019 IEEE International Conference on Image Processing (ICIP)}, 
  title={Joint Vessel Segmentation and Deformable Registration on Multi-Modal Retinal Images Based on Style Transfer}, 
  year={2019},
  volume={},
  number={},
  pages={839-843},
  abstract={In multi-modal retinal image registration task, there are two major challenges, i.e., poor performance in finding correspondence due to inconsistent features, and lack of labeled data for training learning-based models. In this paper, we propose a joint vessel segmentation and deformable registration model based on CNN for this task, built under the framework of weakly supervised style transfer learning and perceptual loss. In vessel segmentation, a style loss guides the model to generate segmentation maps that look authentic, and helps transform images of different modalities into consistent representations. In deformable registration, a content loss helps find dense correspondence for multi-modal images based on their consistent representations, and improves the segmentation results simultaneously. Experiment results show that our model has better performance than other deformable registration methods in both quantitative and visual evaluations, and the segmentation results also help the rigid transformation1.},
  keywords={},
  doi={10.1109/ICIP.2019.8802932},
  ISSN={2381-8549},
  month={Sep.},}

@article{2020TVST_Cavichini_overlay,
    author = {Cavichini, Melina and An, Cheolhong and Bartsch, Dirk-Uwe G. and Jhingan, Mahima and Amador-Patarroyo, Manuel J. and Long, Christopher P. and Zhang, Junkang and Wang, Yiqian and Chan, Alison X. and Madala, Samantha and Nguyen, Truong and Freeman, William R.},
    title = "{Artificial Intelligence for Automated Overlay of Fundus Camera and Scanning Laser Ophthalmoscope Images}",
    journal = {Translational Vision Science & Technology},
    volume = {9},
    number = {2},
    pages = {56-56},
    year = {2020},
    month = {10},
    abstract = "{   The purpose of this study was to evaluate the ability to align two types of retinal images taken on different platforms; color fundus (CF) photographs and infrared scanning laser ophthalmoscope (IR SLO) images using mathematical warping and artificial intelligence (AI).    We collected 109 matched pairs of CF and IR SLO images. An AI algorithm utilizing two separate networks was developed. A style transfer network (STN) was used to segment vessel structures. A registration network was used to align the segmented images to each. Neither network used a ground truth dataset. A conventional image warping algorithm was used as a control. Software displayed image pairs as a 5 Ã— 5 checkerboard grid composed of alternating subimages. This technique permitted vessel alignment determination by human observers and 5 masked graders evaluated alignment by the AI and conventional warping in 25 fields for each image.    Our new AI method was superior to conventional warping at generating vessel alignment as judged by masked human graders (P \\&lt; 0.0001). The average number of good/excellent matches increased from 90.5\\% to 94.4\\% with AI method.    AI permitted a more accurate overlay of CF and IR SLO images than conventional mathematical warping. This is a first step toward developing an AI that could allow overlay of all types of fundus images by utilizing vascular landmarks.    The ability to align and overlay imaging data from multiple instruments and manufacturers will permit better analysis of this complex data helping understand disease and predict treatment.  }",
    issn = {2164-2591},
    doi = {10.1167/tvst.9.2.56},
    url = {https://doi.org/10.1167/tvst.9.2.56},
    eprint = {https://arvojournals.org/arvo/content\_public/journal/tvst/938366/i2164-2591-9-2-56\_1603109585.80174.pdf},
}



